{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Przygotowanie danych do modelowania\n"
   ],
   "metadata": {
    "id": "6GS1L_QCQkbt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wprowadzenie do przetwarzania wstępnego danych tabelarycznych\n",
    "\n",
    "Przetwarzanie wstępne danych tabelarycznych jest kluczowym etapem w procesie budowania modeli uczenia maszynowego. Polega na transformacji surowych danych do formatu, który jest bardziej odpowiedni i zrozumiały dla algorytmów uczenia maszynowego. Dane rzeczywiste często zawierają braki, szumy i niespójności, które mogą negatywnie wpływać na wydajność modelu. Dlatego też, odpowiednie przygotowanie danych przed treningiem jest niezbędne do uzyskania optymalnych wyników.\n",
    "\n",
    "### Setup: importy i dane"
   ],
   "metadata": {
    "id": "SVNXWDweQLXL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import kagglehub"
   ],
   "metadata": {
    "id": "ox_6R5JUU0OI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dane do pobrania z https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data"
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Podłącz Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Wstaw opowiednią ścieżkę do pliku na swoich GDrivie\n",
    "housing_data = pd.read_csv('/content/drive/My Drive/Dydaktyka/OAI-III/housing/train.csv')\n",
    "print(housing_data.shape)\n",
    "print(housing_data.columns)\n",
    "display(housing_data.head())\n",
    "\n",
    "htarget = 'SalePrice'"
   ],
   "metadata": {
    "id": "jZ6bY_5765Du"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fd8ec22"
   },
   "source": [
    "### Podział danych na zbiór treningowy i testowy\n",
    "\n",
    "Podział danych na zbiór treningowy i testowy jest kluczowym krokiem w procesie oceny modelu uczenia maszynowego. Zbiór treningowy jest wykorzystywany do nauki parametrów modelu, podczas gdy zbiór testowy służy do niezależnej oceny wydajności modelu na danych, których model wcześniej nie widział. Taki podział pozwala uniknąć przetrenowania (overfittingu), czyli sytuacji, w której model zbyt dobrze dopasowuje się do danych treningowych, tracąc zdolność generalizacji na nowe dane. Typowo dane są dzielone w proporcjach takich jak 80/20 lub 70/30 na zbiór treningowy i testowy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ed5e0611"
   },
   "source": [
    "!pip install umap-learn"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ff731e30"
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Użyj przetworzonego zbioru danych housing_data\n",
    "# W tym przykładzie użyjemy danych po imputacji braków dla kolumn numerycznych\n",
    "# oraz kolumny docelowej 'SalePrice'\n",
    "# Pamiętaj, że w pełnym potoku przetwarzania wstępnego należałoby użyć danych po wszystkich transformacjach (kodowaniu, skalowaniu)\n",
    "\n",
    "# Zidentyfikuj cechy (wszystkie kolumny oprócz 'SalePrice' i 'Id') i zmienną docelową ('SalePrice')\n",
    "# Użyjmy zbioru danych housing_data po imputacji braków numerycznych\n",
    "X = housing_data.drop(['SalePrice', 'Id'], axis=1)  # Cechy\n",
    "y = housing_data['SalePrice']  # Zmienna docelowa\n",
    "\n",
    "# Podziel dane na zbiór treningowy i testowy\n",
    "# test_size=0.2 oznacza, że 20% danych trafi do zbioru testowego, a 80% do treningowego\n",
    "# random_state=42 zapewnia powtarzalność podziału\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Kształt zbioru treningowego cech (X_train):\", X_train.shape)\n",
    "print(\"Kształt zbioru testowego cech (X_test):\", X_test.shape)\n",
    "print(\"Kształt zbioru treningowego zmiennej docelowej (y_train):\", y_train.shape)\n",
    "print(\"Kształt zbioru testowego zmiennej docelowej (y_test):\", y_test.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21983f37"
   },
   "source": [
    "### Obsługa brakujących wartości\n",
    "\n",
    "Brakujące wartości są powszechnym problemem w rzeczywistych zbiorach danych. Mogą one wystąpić z różnych powodów, takich jak błędy podczas zbierania danych, niedostępność informacji lub błędy wprowadzania danych. Obecność brakujących wartości może prowadzić do błędnych wyników analizy lub obniżyć wydajność modeli uczenia maszynowego. Dlatego ważne jest, aby odpowiednio je obsłużyć przed dalszym przetwarzaniem danych. Typowe metody obsługi brakujących wartości obejmują ich usunięcie (całych wierszy lub kolumn z brakami) lub imputację (zastąpienie brakujących wartości wartościami szacunkowymi, takimi jak średnia, mediana lub wartość najczęściej występująca)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3a038246"
   },
   "source": [
    "# Sprawdź brakujące wartości w zbiorze danych housing_data\n",
    "print(\"Missing values percentage per column in housing_data:\")\n",
    "nulls = X_train.isnull().mean()\n",
    "display((nulls[nulls > 0] * 100).round(2))\n",
    "\n",
    "# Możesz dalej przetwarzać brakujące wartości tutaj, na przykład:\n",
    "\n",
    "# Usuń kolumny z wysokim procentem brakujących wartości:\n",
    "col_above_threshold = (nulls[nulls > 50]).index # np. usuń kolumny z ponad 50% brakujących wartości\n",
    "X_train_cleaned = X_train.drop(col_above_threshold, axis=1)\n",
    "X_test_cleaned = X_test.drop(col_above_threshold, axis=1)\n",
    "\n",
    "# Imputacja brakujących wartości średnią dla kolumn numerycznych:\n",
    "numerical_cols_with_na = X_train_cleaned.select_dtypes(include=np.number).columns[X_train_cleaned.select_dtypes(include=np.number).isnull().any()]\n",
    "for col in numerical_cols_with_na:\n",
    "    X_train_cleaned[col] = X_train_cleaned[col].fillna(X_train_cleaned[col].dropna().mean())\n",
    "    X_test_cleaned[col] = X_test_cleaned[col].fillna(X_train_cleaned[col].dropna().mean())\n",
    "\n",
    "# Imputacja brakujących modą dla kolumn czynnikowych:\n",
    "cathegorical_cols_with_na = X_train_cleaned.select_dtypes(include=np.object_).columns[X_train_cleaned.select_dtypes(include=np.object_).isnull().any()]\n",
    "for col in cathegorical_cols_with_na:\n",
    "    X_train_cleaned[col] = X_train_cleaned[col].fillna(X_train_cleaned[col].dropna().mode().values[0])\n",
    "    X_test_cleaned[col] = X_test_cleaned[col].fillna(X_train_cleaned[col].dropna().mode().values[0])\n",
    "\n",
    "\n",
    "print(\"Missing values percentage per column in housing_data:\")\n",
    "nulls = X_train_cleaned.isnull().mean()\n",
    "display((nulls[nulls > 0] * 100).round(2))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "708ae035"
   },
   "source": [
    "### Kodowanie cech kategorialnych\n",
    "\n",
    "Cechy kategorialne reprezentują dane, które można podzielić na grupy lub kategorie. Algorytmy uczenia maszynowego zazwyczaj wymagają numerycznych danych wejściowych, dlatego konieczne jest przekształcenie cech kategorialnych na format liczbowy. Istnieje wiele metod kodowania cech kategorialnych, a wybór odpowiedniej metody zależy od charakteru danych i algorytmu uczenia maszynowego. Dwie powszechnie stosowane metody to:\n",
    "\n",
    "*   **One-Hot Encoding:** Tworzy nowe kolumny binarne dla każdej unikalnej kategorii w oryginalnej kolumnie. Jest to przydatne, gdy nie ma naturalnego porządku między kategoriami.\n",
    "*   **Label Encoding:** Przypisuje unikalną liczbę całkowitą każdej kategorii. Jest to odpowiednie, gdy istnieje naturalny porządek między kategoriami (np. mały, średni, duży)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "18f66b91"
   },
   "source": [
    "# Zilustruj kodowanie zmiennych kategorialnych na podstawie housing_data\n",
    "\n",
    "# Zidentyfikuj kolumny kategorialne\n",
    "# Użyj type object, ponieważ kolumny kategorialne często są w tym formacie w pandas\n",
    "categorical_cols = X_train_cleaned.select_dtypes(include=np.object_).columns\n",
    "\n",
    "print(\"Kolumny kategorialne w zbiorze danych housing_data:\")\n",
    "print(categorical_cols)\n",
    "\n",
    "for col in categorical_cols:\n",
    "  vals = X_train_cleaned[col].unique()\n",
    "  X_train_cleaned[col] = pd.Categorical(X_train_cleaned[col], categories=vals)\n",
    "  X_test_cleaned[col] = pd.Categorical(X_test_cleaned[col], categories=vals)\n",
    "\n",
    "# Zastosuj One-Hot Encoding do kolumn kategorialnych bez naturalnego porządku (nominalnych)\n",
    "# One-Hot Encoding tworzy nowe kolumny binarne dla każdej unikalnej kategorii.\n",
    "# Jest to odpowiednie, gdy nie ma naturalnego porządku między kategoriami.\n",
    "# Wybierzmy przykładowe kolumny nominalne do demonstracji\n",
    "\n",
    "X_train_cleaned = pd.get_dummies(X_train_cleaned, columns=categorical_cols, drop_first=True) # drop_first=True zapobiega pułapce zmiennej pozornej\n",
    "X_test_cleaned = pd.get_dummies(X_test_cleaned, columns=categorical_cols, drop_first=True) # drop_first=True zapobiega pułapce zmiennej pozornej\n",
    "\n",
    "print(\"\\nDataFrame po One-Hot Encoding (przykładowe kolumny):\")\n",
    "display(X_test_cleaned.head())\n",
    "\n",
    "# Uwaga: W pełnym potoku przetwarzania wstępnego dla zbioru housing_data,\n",
    "# należałoby dokładnie przeanalizować wszystkie kolumny kategorialne\n",
    "# i zastosować odpowiednie metody kodowania (One-Hot, Label z mapowaniem ręcznym, itp.)\n",
    "# oraz obsłużyć brakujące wartości w bardziej zaawansowany sposób, jeśli to konieczne."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce06f16f"
   },
   "source": [
    "### Skalowanie cech numerycznych\n",
    "\n",
    "Skalowanie cech numerycznych jest ważnym etapem przetwarzania wstępnego, ponieważ wiele algorytmów uczenia maszynowego jest wrażliwych na skalę danych wejściowych. Cechy o większych wartościach mogą dominować nad cechami o mniejszych wartościach, co może prowadzić do błędnych lub mniej dokładnych wyników. Skalowanie przekształca wartości cech do określonego zakresu lub rozkładu, zapewniając, że wszystkie cechy mają podobny wpływ na proces uczenia. Dwie popularne metody skalowania to:\n",
    "- standaryzacja (StandardScaler), która skaluje dane tak, aby miały średnią 0 i odchylenie standardowe 1\n",
    "$$\\frac {X - X.mean()} {X.std()}$$\n",
    "- oraz normalizacja (MinMaxScaler), która skaluje dane do określonego zakresu, zazwyczaj od 0 do 1.\n",
    "$$\\frac {X - X.min()} {X.max() - X.min()}$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ae926c4d"
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Zidentyfikuj kolumny numeryczne w zbiorze danych housing_data (z wyłączeniem kolumny 'Id')\n",
    "numerical_cols = X_train_cleaned.select_dtypes(include=np.number).columns.tolist()\n",
    "if 'Id' in numerical_cols:\n",
    "    numerical_cols.remove('Id')\n",
    "\n",
    "# Wybierz tylko kolumny numeryczne do skalowania\n",
    "X_train_cleaned_ = X_train_cleaned[numerical_cols].copy()\n",
    "X_test_cleaned_ = X_test_cleaned[numerical_cols].copy()\n",
    "\n",
    "# Zilustruj standaryzację przy użyciu StandardScaler\n",
    "# Standaryzacja skaluje dane tak, aby miały średnią 0 i odchylenie standardowe 1.\n",
    "# Jest szczególnie użyteczna dla algorytmów, które zakładają, że dane mają rozkład normalny\n",
    "# lub są wrażliwe na wariancję cech, takich jak SVM, regresja logistyczna, czy algorytmy oparte na odległościach.\n",
    "scaler_standard = StandardScaler()\n",
    "X_train_cleaned_standard = pd.DataFrame(scaler_standard.fit_transform(X_train_cleaned_), columns=numerical_cols)\n",
    "X_test_cleaned_standard = pd.DataFrame(scaler_standard.transform(X_test_cleaned_), columns=numerical_cols)\n",
    "\n",
    "print(\"\\nDataFrame po standaryzacji (StandardScaler):\")\n",
    "display(X_train_cleaned_standard.head())\n",
    "\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_cleaned_minmax = pd.DataFrame(scaler_minmax.fit_transform(X_train_cleaned_), columns=numerical_cols)\n",
    "X_test_cleaned_minmax = pd.DataFrame(scaler_minmax.transform(X_test_cleaned_), columns=numerical_cols)\n",
    "\n",
    "print(\"\\nDataFrame po standaryzacji (StandardScaler):\")\n",
    "display(X_train_cleaned_minmax.head())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6338de4"
   },
   "source": [
    "## Redukcja wymiarowości\n",
    "\n",
    "Redukcja wymiarowości jest techniką stosowaną w celu zmniejszenia liczby zmiennych (cech) w zbiorze danych. Jest to przydatne, gdy zbiór danych ma bardzo wiele cech, co może prowadzić do problemu \"przekleństwa wymiarowości\". Redukcja wymiarowości pomaga zmniejszyć złożoność obliczeniową, zredukować szum w danych i ułatwić wizualizację. Popularne metody redukcji wymiarowości to Analiza Głównych Składowych (PCA) i t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60a93be7"
   },
   "source": [
    "### Analiza Głównych Składowych (PCA)\n",
    "\n",
    "PCA (Principal Component Analysis) to popularna technika redukcji wymiarowości, która ma na celu przekształcenie zbioru potencjalnie skorelowanych zmiennych w zbiór nieskorelowanych zmiennych, zwanych głównymi składowymi.\n",
    "\n",
    "Główne składowe są liniowymi kombinacjami oryginalnych cech i są tak konstruowane, aby pierwsza składowa miała największą możliwą wariancję (zawierała najwięcej informacji o zmienności danych), druga składowa miała drugą co do wielkości wariancję i była nieskorelowana z pierwszą, i tak dalej.\n",
    "\n",
    "Proces PCA obejmuje zazwyczaj następujące kroki:\n",
    "1.  **Standaryzacja danych:** Skalowanie danych tak, aby każda cecha miała średnią zero i wariancję jeden. Jest to ważne, ponieważ PCA jest wrażliwe na skalę cech.\n",
    "2.  **Obliczenie macierzy kowariancji (lub korelacji):** Macierz ta pokazuje, jak poszczególne cechy są ze sobą powiązane.\n",
    "3.  **Obliczenie wektorów i wartości własnych:** Wektory własne macierzy kowariancji reprezentują kierunki (główne składowe) w przestrzeni danych, a wartości własne odpowiadają wielkości wariancji w tych kierunkach.\n",
    "4.  **Sortowanie wektorów własnych:** Sortowanie wektorów własnych malejąco według odpowiadających im wartości własnych. Najważniejsze składowe to te z największymi wartościami własnymi.\n",
    "5.  **Wybór podzbioru wektorów własnych:** Wybór K wektorów własnych o największych wartościach własnych, gdzie K to docelowa liczba wymiarów.\n",
    "6.  **Transformacja danych:** Przekształcenie oryginalnych danych na nową przestrzeń zdefiniowaną przez wybrane wektory własne.\n",
    "\n",
    "PCA jest użyteczne do redukcji szumu, kompresji danych, wizualizacji danych wielowymiarowych oraz jako krok wstępny przed zastosowaniem innych algorytmów uczenia maszynowego."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "acba72e4"
   },
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Użyj przetworzonych danych numerycznych z poprzednich kroków (po skalowaniu)\n",
    "# Zmienna housing_data_scaled_standard zawiera numeryczne kolumny housing_data po standaryzacji\n",
    "df_features = X_train_cleaned_standard.copy()\n",
    "\n",
    "# Zastosuj PCA\n",
    "# Zredukuj do 2 komponentów dla wizualizacji\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(df_features)\n",
    "df_pca = pd.DataFrame(data = principal_components, columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "print(\"\\nDataFrame po PCA (2 główne składowe):\")\n",
    "display(df_pca.head())\n",
    "\n",
    "# Opcjonalnie: Wizualizacja wyników PCA (jeśli masz więcej punktów danych i chcesz zwizualizować klastry lub rozkład)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df_pca['principal component 1'], df_pca['principal component 2'])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA zbioru danych housing_data (kolumny numeryczne)')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "508c01f8"
   },
   "source": [
    "#### Interpretacja komponentów głównych\n",
    "\n",
    "Tutaj można dodać interpretację znaczenia poszczególnych komponentów głównych w kontekście analizowanych danych. Na przykład, które oryginalne cechy mają największy wpływ na pierwszy komponent główny? Co reprezentują te komponenty?"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for component in pca.components_:\n",
    "  plt.bar(df_features.columns, component)\n",
    "  plt.xticks(rotation=90)\n",
    "  plt.title('New variable in PCA')\n",
    "  plt.ylabel('weight')\n",
    "  plt.show()"
   ],
   "metadata": {
    "id": "rY4-xV-QYJmi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6471634e"
   },
   "source": [
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "Wyobraź sobie, że masz zbiór danych z wieloma cechami, a chcesz zobaczyć, jak te punkty danych są ze sobą powiązane w przestrzeni o mniejszej liczbie wymiarów (np. na płaszczyźnie 2D). PCA pomaga znaleźć najważniejsze kierunki zmienności, ale czasami nie najlepiej zachowuje lokalne relacje między punktami (czyli to, które punkty są \"sąsiadami\").\n",
    "\n",
    "t-SNE działa trochę inaczej. Skupia się na tym, żeby punkty, które są blisko siebie w oryginalnej przestrzeni o wielu wymiarach, pozostały blisko siebie na wykresie 2D (lub 3D). Jednocześnie stara się, żeby punkty, które są daleko od siebie w oryginalnej przestrzeni, pozostały daleko na wykresie.\n",
    "\n",
    "Można to sobie wyobrazić tak, jakbyś próbował rozłożyć trójwymiarową kulę na płaską kartkę papieru. Nie da się tego zrobić idealnie bez zniekształceń. t-SNE próbuje zminimalizować te zniekształcenia, koncentrując się na zachowaniu \"sąsiedztwa\".\n",
    "\n",
    "Jak to robi?\n",
    "1. Najpierw oblicza, jak bardzo \"podobne\" są do siebie wszystkie pary punktów w oryginalnej, wielowymiarowej przestrzeni. Punkty blisko siebie są bardzo podobne, punkty daleko - mało podobne.\n",
    "2. Następnie tworzy mapę tych punktów w przestrzeni o mniejszej liczbie wymiarów (np. 2D) i też oblicza ich \"podobieństwo\".\n",
    "3. Na koniec, t-SNE przesuwa punkty na mapie 2D tak, aby \"podobieństwo\" punktów na mapie było jak najbardziej zbliżone do \"podobieństwa\" punktów w oryginalnej przestrzeni. Robi to iteracyjnie, czyli krok po kroku poprawia rozmieszczenie punktów.\n",
    "\n",
    "W efekcie dostajemy wykres, na którym skupiska punktów reprezentują grupy podobnych obiektów. t-SNE jest świetne do wizualizacji złożonych danych i odkrywania ukrytych struktur lub klastrów, ale trudniej jest interpretować same osie wykresu tak jak w PCA."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Użyj przetworzonych danych numerycznych z poprzednich kroków (po skalowaniu)\n",
    "# Zmienna housing_data_scaled_standard zawiera numeryczne kolumny housing_data po standaryzacji\n",
    "df_features = X_train_cleaned_standard.copy()\n",
    "\n",
    "# Zastosuj t-SNE\n",
    "# Zredukuj do 2 komponentów dla wizualizacji\n",
    "# Wartości perplexity i max_iter mogą wymagać dostrojenia w zależności od danych\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=300)\n",
    "principal_components_tsne = tsne.fit_transform(df_features)\n",
    "df_tsne = pd.DataFrame(data = principal_components_tsne, columns = ['tsne component 1', 'tsne component 2'])\n",
    "\n",
    "print(\"\\nDataFrame po t-SNE (2 komponenty):\")\n",
    "display(df_tsne.head())\n",
    "\n",
    "# Wizualizacja wyników t-SNE\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df_tsne['tsne component 1'], df_tsne['tsne component 2'])\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title('t-SNE zbioru danych housing_data (kolumny numeryczne)')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "EYgbDVV9RKn4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62c16e52"
   },
   "source": [
    "### UMAP (Uniform Manifold Approximation and Projection) - intuicyjnie\n",
    "\n",
    "UMAP to kolejna technika redukcji wymiarowości, podobna do t-SNE, ale często szybsza i lepiej zachowująca globalną strukturę danych.\n",
    "\n",
    "Wyobraź sobie, że Twoje dane o wielu wymiarach leżą na skomplikowanej, pofałdowanej powierzchni w tej wielowymiarowej przestrzeni. UMAP próbuje znaleźć sposób, żeby \"rozłożyć\" tę pofałdowaną powierzchnię na płasko (np. na 2D), zachowując jednocześnie odległości między punktami.\n",
    "\n",
    "Jak to robi?\n",
    "1. **Buduje wykres \"sąsiadów\":** Najpierw UMAP tworzy wykres, gdzie każdy punkt danych jest połączony z jego najbliższymi sąsiadami w oryginalnej, wielowymiarowej przestrzeni. Siła połączenia zależy od tego, jak blisko są te punkty.\n",
    "2. **Tworzy wykres 2D:** Następnie UMAP próbuje zbudować podobny wykres w przestrzeni o mniejszej liczbie wymiarów (np. 2D).\n",
    "3. **Dopasowuje wykresy:** UMAP dopasowuje rozmieszczenie punktów na wykresie 2D tak, aby struktura \"sąsiadów\" na wykresie 2D była jak najbardziej podobna do struktury \"sąsiadów\" w oryginalnej, wielowymiarowej przestrzeni.\n",
    "\n",
    "Podobnie jak t-SNE, UMAP jest świetne do wizualizacji i odkrywania grup w danych. Często daje lepsze wyniki w zachowaniu zarówno lokalnej, jak i globalnej struktury danych w porównaniu do t-SNE.\n",
    "\n",
    "W skrócie, UMAP próbuje stworzyć \"mapę\" Twoich wielowymiarowych danych w niższej wymiarowości, koncentrując się na zachowaniu relacji między punktami (kto jest czyim sąsiadem i jak blisko)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6071e62a"
   },
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Użyj przetworzonych danych numerycznych z poprzednich kroków (po skalowaniu)\n",
    "# Zmienna housing_data_scaled_standard zawiera numeryczne kolumny housing_data po standaryzacji\n",
    "df_features = X_train_cleaned_standard.copy()\n",
    "\n",
    "# Zastosuj UMAP\n",
    "# Zredukuj do 2 komponentów dla wizualizacji\n",
    "# Parametry n_neighbors i min_dist mogą wymagać dostrojenia\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "embedding = reducer.fit_transform(df_features)\n",
    "\n",
    "df_umap = pd.DataFrame(embedding, columns=['umap component 1', 'umap component 2'])\n",
    "\n",
    "print(\"\\nDataFrame po UMAP (2 komponenty):\")\n",
    "display(df_umap.head())\n",
    "\n",
    "# Wizualizacja wyników UMAP\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df_umap['umap component 1'], df_umap['umap component 2'])\n",
    "plt.xlabel('UMAP Component 1')\n",
    "plt.ylabel('UMAP Component 2')\n",
    "plt.title('UMAP zbioru danych housing_data (kolumny numeryczne)')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95f3ee18"
   },
   "source": [
    "## Ćwiczenia\n",
    "\n",
    "Po zapoznaniu się z podstawowymi technikami przetwarzania wstępnego danych tabelarycznych, proponujemy następujące ćwiczenia, które pozwolą utrwalić zdobytą wiedzę i umiejętności:\n",
    "\n",
    "**Zadanie 1: Analiza i Przetwarzanie Danych Titanic**\n",
    "\n",
    "Wykorzystując zbiór danych Titanic (https://www.kaggle.com/datasets/yasserh/titanic-dataset), przeprowadź kompleksowe przetwarzanie wstępne danych, uwzględniając poznane techniki:\n",
    "\n",
    "1.  **Ładowanie i wstępna analiza**: Załaduj dane do DataFrame i przeprowadź wstępną analizę:\n",
    "    *   Sprawdź rozmiar zbioru danych (liczba wierszy i kolumn).\n",
    "    *   Wyświetl pierwsze kilka wierszy, aby zapoznać się ze strukturą danych.\n",
    "    *   Sprawdź typy danych poszczególnych kolumn.\n",
    "    *   Zidentyfikuj kolumny numeryczne i kategoryczne.\n",
    "2.  **Obsługa brakujących wartości**:\n",
    "    *   Zidentyfikuj kolumny z brakującymi wartościami i wyświetl procent braków dla każdej z nich.\n",
    "    *   Zastosuj odpowiednie strategie imputacji dla kolumn numerycznych (np. średnia, mediana) i kategorialnych (np. moda, stała wartość, np. \"Missing\"). Uzasadnij swój wybór dla każdej kolumny.\n",
    "3.  **Kodowanie cech kategorialnych**:\n",
    "    *   Zastosuj One-Hot Encoding do kolumn kategorialnych, które nie mają naturalnego porządku.\n",
    "    *   Zastosuj Label Encoding lub inne odpowiednie kodowanie (np. mapowanie ręczne) do kolumn kategorialnych, które mają naturalny porządek (jeśli takie występują w zbiorze danych Titanic).\n",
    "4.  **Skalowanie cech numerycznych**:\n",
    "    *   Zastosuj standaryzację (StandardScaler) do wszystkich kolumn numerycznych.\n",
    "    *   Zastosuj normalizację (MinMaxScaler) do tych samych kolumn numerycznych w osobnym etapie. Porównaj rozkłady danych po obu transformacjach (np. używając histogramów).\n",
    "5.  **Podział danych**: Podziel przetworzone dane na zbiór treningowy i testowy w proporcji 80/20 lub 70/30.\n",
    "\n",
    "**Zadanie 2: Redukcja Wymiarowości i Wizualizacja**\n",
    "\n",
    "Kontynuuj pracę na przetworzonym zbiorze danych Titanic (po zastosowaniu wszystkich kroków przetwarzania wstępnego):\n",
    "\n",
    "1.  **Zastosowanie PCA**: Zastosuj PCA do przetworzonych danych, redukując wymiarowość do 2 lub 3 komponentów. Zwizualizuj wyniki PCA na wykresie punktowym. Spróbuj zinterpretować znaczenie pierwszych komponentów głównych, analizując wagi oryginalnych cech.\n",
    "2.  **Zastosowanie t-SNE**: Zastosuj t-SNE do przetworzonych danych, redukując wymiarowość do 2 komponentów. Zwizualizuj wyniki t-SNE na wykresie punktowym. Czy widzisz wyraźne skupiska punktów odpowiadające np. klasie przeżycia?\n",
    "3.  **Zastosowanie UMAP**: Zastosuj UMAP do przetworzonych danych, redukując wymiarowość do 2 komponentów. Zwizualizuj wyniki UMAP na wykresie punktowym. Porównaj wizualizacje z t-SNE i PCA - która metoda lepiej oddaje strukturę danych w niskowymiarowej przestrzeni?\n",
    "4.  **Dyskusja**: Napisz krótkie podsumowanie porównujące zastosowane metody redukcji wymiarowości w kontekście zbioru danych Titanic. Kiedy warto zastosować każdą z nich? Jakie są ich zalety i wady w tym konkretnym przypadku?\n",
    "\n",
    "Powodzenia!"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "K2h2B_2ZXai5"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
