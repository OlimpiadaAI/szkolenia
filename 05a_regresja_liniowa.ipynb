{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e6ddcd01",
      "metadata": {
        "id": "e6ddcd01"
      },
      "source": [
        "Zalecamy nie czytać notatników na githubie, ze względu na źle wyświetlające się wizualizacje i brak możliwości uruchamiania kodu. Polecamy otworzyć notatnik w google colab.\n",
        "\n",
        "# **Olimpiada AI - kurs wprowadzający 2025 - Wykład 05A**\n",
        "\n",
        "\"Wstęp do Regresji Liniowej\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b08a622c",
      "metadata": {
        "id": "b08a622c"
      },
      "source": [
        "## Regresja Liniowa - Intuicja\n",
        "\n",
        "Cała istota Uczenia Maszynowego polega na szukaniu \"ukrytych\" funkcji, które przekształcają nam jedną zmienną (wejściową) w drugą (wyjściową). Wyobraźcie sobie, że chcemy przewidzieć wynik ze sprawdzianu (zmienna wyjściowa) z matematyki na podstawie czasu poświęconego na naukę (zmienna wejściowa).\n",
        "\n",
        "W liceum uczyliście się na pewno wzoru na funkcję liniową:\n",
        "$$y = ax + b$$\n",
        "\n",
        "Teraz wyobraźmy sobie, że to jes prawdziwa funkcją opisująca ile czasu trzeba poświęcić na naukę, gdzie:\n",
        "\n",
        "  * $y$ to wynik ze sprawdzianu\n",
        "  * $x$ to nasza dana wejściowa - godziny nauki\n",
        "  * $a$ i $b$ to parametry (współczynniki), których szukamy\n",
        "\n",
        "Stwórzmy przykładowe dane:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a92224c7",
      "metadata": {
        "id": "a92224c7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ustawiamy ziarno losowości, żeby każdy miał ten sam wynik\n",
        "torch.manual_seed(2025)\n",
        "\n",
        "def generate_student_data(n=50):\n",
        "    # Generujemy losowe godziny nauki (od 0 do 100 godzin)\n",
        "    hours_studied = torch.rand(n) * 100\n",
        "\n",
        "    # Prawdziwa zależność (ukryta przed modelem):\n",
        "    # Każda godzina nauki daje średnio 0.8 punktu, startujemy z poziomu 10pkt\n",
        "    scores = 0.8 * hours_studied + 10\n",
        "\n",
        "    # Dodajemy \"szum\": stres, szczęście, dyspozycja dnia (+/- 15pkt)\n",
        "    # noise = torch.randn(n) * 15\n",
        "    # scores += noise\n",
        "\n",
        "    return hours_studied, scores\n",
        "\n",
        "X_hours, y_scores = generate_student_data()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_hours, y_scores, color='blue', label='Wyniki różnych uczniów')\n",
        "plt.title(\"Zależność: Czas nauki a wynik ze sprawdzianu z matematyki\")\n",
        "plt.xlabel(\"Godziny nauki (x)\")\n",
        "plt.ylabel(\"Wynik % (y)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a95d8a8",
      "metadata": {
        "id": "8a95d8a8"
      },
      "source": [
        "Spróbujmy teraz wykonać bardzo prostą esymację - po prostu przypisać każdemu uczniowi średnią wartość z testu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4436235f",
      "metadata": {
        "id": "4436235f"
      },
      "outputs": [],
      "source": [
        "def mean_estimate(a_guess, b_guess, X, y, plot_error=False):\n",
        "    # Przewidywania naszej \"zgadywanej\" linii\n",
        "    y_pred = a_guess * X + b_guess\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(X, y, color='blue', label='Dane prawdziwe')\n",
        "    plt.plot(X, y_pred, color='red', linewidth=2, label=f'Nasza linia: y = {a_guess}x + {b_guess}')\n",
        "\n",
        "    # Rysowanie błędów (Reszt)\n",
        "\n",
        "\n",
        "    total_error = 0\n",
        "    for i in range(len(X)):\n",
        "        # Rysujemy pionową kreskę od punktu do linii\n",
        "        if plot_error:\n",
        "            plt.plot([X[i], X[i]], [y[i], y_pred[i]], 'g--', alpha=0.5)\n",
        "        # Sumujemy błędy (kwadrat różnicy, żeby unikać liczb ujemnych)\n",
        "        total_error += abs(y[i]-y_pred[i])\n",
        "\n",
        "    plt.title(f\"Wizualizacja błędu (Loss). Suma błędów: {total_error:.0f}\")\n",
        "    plt.xlabel(\"Godziny nauki\")\n",
        "    plt.ylabel(\"Wynik (w punktach)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Spróbujmy zgadnąć: a=0 (brak wpływu nauki), b=50 (każdy dostaje 50pkt)\n",
        "mean_estimate(a_guess=0.0, b_guess=50, X=X_hours, y=y_scores, plot_error=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "122722d1",
      "metadata": {
        "id": "122722d1"
      },
      "source": [
        "## Jak dobra jest nasza estymacja?\n",
        "\n",
        "My widzimy na oko, czy linia jest dobra czy nie i gdzie mniej-więcej powinna przebiegać idealna linia. Komputer tego nie widzi. Komputer potrafi tylko liczyć wartości i na ich podstawie podejmować \"decyzje\".\n",
        "\n",
        "W Machine Learningu ocenę, jak dobra jest nasza estymacja nazywamy \"Funkcją Kosztu\" (Loss Function). W tym przypadku to po prostu suma błędów - odległości od poszczególnych pomiarów - jakie popełnia nasza estymacja (linia).\n",
        "\n",
        "**Ważne** - zauważ, że tutaj odległość od punktów liczymy tylko względem osi Y, to nie jest odległość liczona pod kątem prostym do naszej estymacji! \n",
        "\n",
        "Sprawdźmy jak to wygląda na przykładzie bardzo prostej (i niezbyt przydatnej estymacji)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf35abff",
      "metadata": {
        "id": "cf35abff"
      },
      "outputs": [],
      "source": [
        "mean_estimate(a_guess=0.0, b_guess=50, X=X_hours, y=y_scores, plot_error=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2045e8b6",
      "metadata": {
        "id": "2045e8b6"
      },
      "source": [
        "Zielone kreski to błędy. Celem algorytmów uczących jest tak manipulować `a` i `b`, żeby suma długości zielonych kresek była jak najmniejsza."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e2131c9",
      "metadata": {
        "id": "9e2131c9"
      },
      "source": [
        "## Machine Learning w akcji (PyTorch)\n",
        "\n",
        "Teraz użyjemy biblioteki *PyTorch*, aby komputer sam znalazł idealną linię. Użyjemy metody zwanej Gradient Descent (spadek wzdłuż gradientu) o której powiemy sobie więcej za chwilę."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89bba11b",
      "metadata": {
        "id": "89bba11b"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# 1. Przygotowanie danych (format wymagany przez PyTorch)\n",
        "X_tensor = X_hours.view(-1, 1) # Zmieniamy kształt na kolumnę\n",
        "y_tensor = y_scores.view(-1, 1)\n",
        "\n",
        "# 2. Definiujemy parametry a i b (na początku losowe!)\n",
        "# requires_grad=True oznacza: \"PyTorch, śledź te zmienne i mów mi, jak je zmieniać\"\n",
        "a = torch.randn(1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "# 3. Ustawienia uczenia\n",
        "learning_rate = 0.0001 # Jak duże kroki robimy\n",
        "optimizer = torch.optim.SGD([a, b], lr=learning_rate)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "print(\"Startujemy naukę...\")\n",
        "print(f\"Początkowe losowe parametry: a = {a.item():.2f}, b = {b.item():.2f}\")\n",
        "\n",
        "# 4. Pętla uczenia (Trening)\n",
        "errors_history = []\n",
        "\n",
        "for epoch in range(1000):\n",
        "    # Krok 1: Model przewiduje wynik\n",
        "    y_pred = a * X_tensor + b\n",
        "\n",
        "    # Krok 2: Liczymy błąd (Loss)\n",
        "    loss = loss_fn(y_pred, y_tensor)\n",
        "    errors_history.append(loss.item())\n",
        "\n",
        "    # Krok 3: Gradient Backward (na razie magia, opowiemy sobie o tym za chwilę)\n",
        "    # PyTorch liczy pochodne - sprawdza w którą stronę zmienić a i b, żeby błąd zmalał\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Krok 4: Aktualizacja parametrów\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        print(f\"Epoka {epoch}: Błąd średni = {loss.item():.2f}\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"NAUCZONE parametry: a = {a.item():.2f}, b = {b.item():.2f}\")\n",
        "print(\"(Pamiętacie? Prawdziwe ukryte 'a' wynosiło 0.8, a 'b' 0.1)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc249f12",
      "metadata": {
        "id": "fc249f12"
      },
      "source": [
        "### Sprawdźmy wynik graficznie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c1885c",
      "metadata": {
        "id": "10c1885c"
      },
      "outputs": [],
      "source": [
        "# Wizualizacja po treningu\n",
        "mean_estimate(a_guess=a.item(), b_guess=b.item(), X=X_hours, y=y_scores, plot_error=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9930fdf",
      "metadata": {
        "id": "c9930fdf"
      },
      "source": [
        "## Wielomiany\n",
        "\n",
        "Regresja liniowa zakłada, że funkcja, którą modelujemy to prosta. Ale wyobraźcie sobie na przykład drogę hamowania samochodu albo tor lotu piłki - to są przykłady krzywych.\n",
        "\n",
        "Jeśli użyjemy zwykłej linii do opisania zakrętu, nasz błąd będzie bardzo duży, dlatego musimy sięgnąć po trochę potężniejsze narzędzie. Żeby przybliżyć krzywą za pomocą regresji liniowej musimy użyć pewnego tricku - do naszej estymacji dodajemy potęgi $x^2, x^3...$. Algorytm jest ten sam, to wciąż regresja, ale tworzy nam Wielomian."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c233d2a",
      "metadata": {
        "id": "8c233d2a"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Dane: Czas (t) vs Pozycja samochodu\n",
        "t = np.linspace(0, 1, 100).reshape(-1, 1)\n",
        "\n",
        "# Prawdziwy ruch jest skomplikowany (wielomian)\n",
        "# Wyobraźcie sobie, że samochód przyspiesza i skręca\n",
        "y_position = t**3 - 0.5 * t**2 + 0.2 * t\n",
        "y_position += np.random.normal(0, 0.02, size=y_position.shape) # Szum\n",
        "\n",
        "# 1. Próba dopasowania PROSTEJ LINII\n",
        "model_linear = LinearRegression()\n",
        "model_linear.fit(t, y_position)\n",
        "y_pred_line = model_linear.predict(t)\n",
        "\n",
        "# 2. Próba dopasowania WIELOMIANU\n",
        "# Zamieniamy zwykłe [t] na [t, t^2, t^3]\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "t_poly = poly.fit_transform(t)\n",
        "\n",
        "model_poly = LinearRegression()\n",
        "model_poly.fit(t_poly, y_position)\n",
        "y_pred_poly = model_poly.predict(t_poly)\n",
        "\n",
        "# Wykres\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(t, y_position, color='gray', s=10, label='Zaszumione odczyty pozycji z drogi hamowania')\n",
        "plt.plot(t, y_pred_line, color='red', linestyle='--', label='Model Liniowy')\n",
        "plt.plot(t, y_pred_poly, color='green', linewidth=3, label='Model Wielomianowy')\n",
        "\n",
        "plt.title(\"Dlaczego potrzebujemy bardziej ekspresywnych modeli?\")\n",
        "plt.xlabel(\"Czas\")\n",
        "plt.ylabel(\"Pozycja\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55df7943",
      "metadata": {
        "id": "55df7943"
      },
      "source": [
        "## Dlaczego potrzebujemy bardziej ekspresyjnych modeli?\n",
        "\n",
        "W praktyce, takie funkcje są dużo bardziej skomplikowane niż proste lub nawet wielomiany. Każdy problem w uczeniu maszynowym próbujemy rozwiązać funkcją! Nawet klasyfikcaję obrazów czy generowanie tekstów za pomocą ChatuGPT. Dlatego gdy trenujemy ogromne sieci neuronowe (jak ChatGPT czy systemy rozpoznawania twarzy), one są trenowane tak samo jak widzieliście tutaj. Próbują estymować funkcję, która za pomocą danych wejściowych chce przewidzieć dane wyjściowe. To znaczy, że funkcje *przekształcają przestrzeń* wejściową, tak żeby uzyskać wyjściową."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
